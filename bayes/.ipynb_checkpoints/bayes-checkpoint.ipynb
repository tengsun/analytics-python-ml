{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load phrase data and class\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    # 0 - normal, 1 - offensive\n",
    "    classVector = [0, 1, 0, 1, 0, 1]\n",
    "    return postingList, classVector\n",
    "\n",
    "# build vocabulary list\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for phrase in dataSet:\n",
    "        # combine the two sets\n",
    "        vocabSet = vocabSet | set(phrase)\n",
    "    return list(vocabSet)\n",
    "\n",
    "# mark phrase in vocabulary list\n",
    "def phrase2Vector(vocabList, inputPhrase):\n",
    "    phraseVector = [0] * len(vocabList)\n",
    "    for word in inputPhrase:\n",
    "        if word in vocabList:\n",
    "            phraseVector[vocabList.index(word)] = 1\n",
    "        # else:\n",
    "        #    print \"the word %s is not in my vocabulary.\" %word\n",
    "    return phraseVector\n",
    "\n",
    "# set-of-words -> bag-of-words\n",
    "def phrase2BagVector(vocabList, inputPhrase):\n",
    "    phraseVector = [0] * len(vocabList)\n",
    "    for word in inputPhrase:\n",
    "        if word in vocabList:\n",
    "            phraseVector[vocabList.index(word)] += 1\n",
    "        # else:\n",
    "        #    print \"the word %s is not in my vocabulary.\" %word\n",
    "    return phraseVector\n",
    "\n",
    "dataSet, labels = loadDataSet()\n",
    "vocabList = createVocabList(dataSet)\n",
    "print vocabList\n",
    "vector = phrase2Vector(vocabList, dataSet[0])\n",
    "print vector\n",
    "bagVector = phrase2BagVector(vocabList, dataSet[0])\n",
    "print bagVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08  0.08  0.08  0.04  0.04  0.08  0.08  0.08  0.04  0.08  0.08  0.08\n",
      "  0.08  0.04  0.04  0.12  0.04  0.04  0.08  0.04  0.08  0.08  0.04  0.08\n",
      "  0.08  0.08  0.04  0.08  0.04  0.08  0.08  0.16]\n",
      "[ 0.05  0.05  0.05  0.1   0.1   0.05  0.05  0.05  0.1   0.1   0.05  0.05\n",
      "  0.05  0.1   0.1   0.1   0.1   0.1   0.05  0.15  0.05  0.1   0.1   0.05\n",
      "  0.15  0.05  0.2   0.05  0.1   0.05  0.05  0.05]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# unified train matrix, and relevant categories\n",
    "def trainNaiveBayes(trainMatrix, trainCategory):\n",
    "    numTrainPhrases = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    \n",
    "    # offensive phrase's category value is 1\n",
    "    pOffensive = sum(trainCategory) / float(numTrainPhrases)\n",
    "    \n",
    "    # set word initial and sum to 1.0 to avoid zero\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    p0Sum = 1.0\n",
    "    p1Sum = 1.0\n",
    "    \n",
    "    # count words vector and sum value in 0 and 1 category\n",
    "    for i in range(numTrainPhrases):\n",
    "        if trainCategory[i] == 0:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Sum += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Sum += sum(trainMatrix[i])\n",
    "    \n",
    "    # calculate p(w|ci) vector\n",
    "    p0Vector = p0Num / p0Sum\n",
    "    p1Vector = p1Num / p1Sum\n",
    "    \n",
    "    return p0Vector, p1Vector, pOffensive\n",
    "\n",
    "trainMat = []\n",
    "for post in dataSet:\n",
    "    trainMat.append(phrase2Vector(vocabList, post))\n",
    "p0V, p1V, pOff = trainNaiveBayes(trainMat, labels)\n",
    "print p0V\n",
    "print p1V\n",
    "print pOff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as 0\n",
      "['stupid', 'garbage'] classified as 1\n"
     ]
    }
   ],
   "source": [
    "def classifyNaiveBayes(inputVector, p0Vec, p1Vec, pClass1):\n",
    "    # p(w|ci) * p(ci) -> ln(a*b) = ln(a) + ln(b)\n",
    "    # ln(p(w|ci)) = ln(p(w0|ci)) + ln(p(w1|ci)) + ... + ln(p(wn|ci))\n",
    "    p1 = sum(inputVector * np.log(p1Vec)) + np.log(pClass1)\n",
    "    p0 = sum(inputVector * np.log(p0Vec)) + np.log(1 - pClass1)\n",
    "    # print \"p0 = %f, p1 = %f\" %(p0, p1)\n",
    "    \n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def testNaiveBayes():\n",
    "    phrases, classes = loadDataSet()\n",
    "    vocabList = createVocabList(phrases)\n",
    "    trainMat = []\n",
    "    for phrase in phrases:\n",
    "        trainMat.append(phrase2Vector(vocabList, phrase))\n",
    "    p0V, p1V, pOff = trainNaiveBayes(trainMat, classes)\n",
    "\n",
    "    testPhrase = ['love', 'my', 'dalmation']\n",
    "    testVector = phrase2Vector(vocabList, testPhrase)\n",
    "    print \"%r classified as %d\" %(testPhrase, classifyNaiveBayes(testVector, p0V, p1V, pOff))\n",
    "    \n",
    "    testPhrase = ['stupid', 'garbage']\n",
    "    testVector = phrase2Vector(vocabList, testPhrase)\n",
    "    print \"%r classified as %d\" %(testPhrase, classifyNaiveBayes(testVector, p0V, p1V, pOff))\n",
    "\n",
    "testNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train number set: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 22, 23, 26, 28, 29, 30, 31, 33, 34, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "test number set: [32, 35, 17, 37, 25, 21, 24, 38, 20, 27]\n",
      "No. 32 analysis result is 0, but actually is 1\n",
      "No. 27 analysis result is 1, but actually is 0\n",
      "the error rate is 0.200000\n"
     ]
    }
   ],
   "source": [
    "## spam email test\n",
    "def textParse(longText):\n",
    "    import re\n",
    "    tokens = re.split(\"\\\\W*\", longText)\n",
    "    tokens = [tk.lower() for tk in tokens if len(tk) > 2]\n",
    "    return tokens\n",
    "\n",
    "def spamEmailTest():\n",
    "    docList = []; classList = []; fullText = []\n",
    "    \n",
    "    # read 25 files in spam and ham folder, total is 50\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('data/email/spam/%d.txt' %i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('data/email/ham/%d.txt' %i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    \n",
    "    # build train number set (size=40) and test number set (size=10)\n",
    "    trainNumSet = range(50); testNumSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainNumSet)))\n",
    "        testNumSet.append(trainNumSet[randIndex])\n",
    "        del(trainNumSet[randIndex])\n",
    "    \n",
    "    # build train matrix (size=40)\n",
    "    print 'train number set: %s' %trainNumSet\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainNumSet:\n",
    "        trainMat.append(phrase2BagVector(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNaiveBayes(trainMat, trainClasses)\n",
    "    \n",
    "    errorCount = 0\n",
    "    print 'test number set: %s' %testNumSet\n",
    "    for docIndex in testNumSet:\n",
    "        wordVector = phrase2BagVector(vocabList, docList[docIndex])\n",
    "        resultClass = classifyNaiveBayes(wordVector, p0V, p1V, pSpam)\n",
    "        actualClass = classList[docIndex] \n",
    "        if resultClass != actualClass:\n",
    "            errorCount += 1\n",
    "            print \"No. %d analysis result is %d, but actually is %d\" \\\n",
    "                %(docIndex, resultClass, actualClass)\n",
    "    print \"the error rate is %f\" %(float(errorCount)/len(testNumSet))\n",
    "\n",
    "spamEmailTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## words and location classification\n",
    "\n",
    "def calcMostFreq(vocabList, fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token] = fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.iteritems(), key = operator.itemgetter(1), \\\n",
    "                        reverse = True)\n",
    "    return sortedFreq[:30]\n",
    "\n",
    "def localWords(feed1, feed0):\n",
    "    import feedparser\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    \n",
    "    # prepare doc list and full text array\n",
    "    minLen = min(len(feed1['entries']), len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "     \n",
    "    # create vocabulary list and top 30 frequent words\n",
    "    vocabList = createVocabList(docList)\n",
    "    top30Words = calcMostFreq(vocabList, fullText)\n",
    "    print 'vocabulary list length: %d' %len(vocabList)\n",
    "     \n",
    "    # remove top 30 frequent words from vocabulary list\n",
    "    # comment or uncomment to see the result differences\n",
    "    for word in top30Words:\n",
    "        if word[0] in vocabList:\n",
    "            vocabList.remove(word[0])\n",
    "     \n",
    "    # pick out 10 from training set randomly as test set\n",
    "    trainingSet = range(2 * minLen)\n",
    "    testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "         \n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    print 'train number set: %s' %trainingSet\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(phrase2BagVector(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pLoc1 = trainNaiveBayes(trainMat, trainClasses)\n",
    "     \n",
    "    errorCount = 0\n",
    "    print 'test number set: %s' %testSet\n",
    "    for docIndex in testSet:\n",
    "        wordVector = phrase2BagVector(vocabList, docList[docIndex])\n",
    "        resultClass = classifyNaiveBayes(wordVector, p0V, p1V, pLoc1)\n",
    "        actualClass = classList[docIndex]\n",
    "        if resultClass != actualClass:\n",
    "            errorCount += 1\n",
    "            print \"No. %d analysis result is %d, but actually is %d\" \\\n",
    "                %(docIndex, resultClass, actualClass)\n",
    "    print \"the error rate is %f\" %(float(errorCount)/len(testSet))\n",
    "    return vocabList, p0V, p1V\n",
    "\n",
    "def getTopWords():\n",
    "    import operator\n",
    "    import feedparser\n",
    "    ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "    sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "    vocabList, pSF, pNY = localWords(ny, sf)\n",
    "    \n",
    "    topSF = []\n",
    "    topNY = []\n",
    "    for i in range(len(pSF)):\n",
    "        if pSF[i] > 0.006: topSF.append((vocabList[i], pSF[i]))\n",
    "        if pNY[i] > 0.006: topNY.append((vocabList[i], pNY[i]))\n",
    "    \n",
    "    sortedSF = sorted(topSF, key = lambda pair: pair[1], reverse = True)\n",
    "    print '--- SF Bay (%d) ---' % len(sortedSF)\n",
    "    topSFWords = []\n",
    "    for item in sortedSF:\n",
    "        topSFWords.append(item[0])\n",
    "    print topSFWords\n",
    "    \n",
    "    sortedNY = sorted(topNY, key = lambda pair: pair[1], reverse = True)\n",
    "    print '--- New York (%d) ---' % len(sortedNY)\n",
    "    topNYWords = []\n",
    "    for item in sortedNY:\n",
    "        topNYWords.append(item[0])\n",
    "    print topNYWords\n",
    "\n",
    "# getTopWords()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
